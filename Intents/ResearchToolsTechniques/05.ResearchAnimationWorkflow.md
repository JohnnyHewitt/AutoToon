# 5. Research Animation Workflow

**Goal**: How 2D animation actually works in the chosen tool: rigging, tweening, lip sync, scene composition. What can be scripted vs. what needs manual work.
**Est.**: ≤2 hours
**Dependencies**: 02

## Steps
- [x] Research core 2D animation concepts: rigging, tweening, keyframes, lip sync
- [x] Document how the top candidate tool handles each
- [x] Identify which steps are scriptable/automatable
- [x] Assess what remains manual and how to minimize it

## Definition of Done
- [x] Animation workflow documented for chosen tool
- [x] Scriptable vs. manual steps clearly mapped
- [x] Lip sync and scene composition approaches identified

## Findings

### Blender GP Animation Concepts

| Concept | How Blender GP Does It | Scriptable? |
|---------|----------------------|-------------|
| **Rigging** | Armature bones drive GP objects. Same bone system as 3D. | Yes — `bpy.data.armatures`, `bpy.ops.object.parent_set` |
| **Keyframes** | Standard keyframe system on GP layers/frames | Yes — `bpy.data.actions`, keyframe insert via API |
| **Tweening** | Interpolation between keyframes (GP interpolation tool) | Yes — `bpy.ops.gpencil.interpolate_sequence` |
| **Lip sync** | No built-in lip sync. Requires shape keys or drawing substitution | Scriptable via shape keys or layer visibility switching |
| **Scene composition** | Standard Blender scene/camera system | Yes — full `bpy.data.scenes`, `bpy.data.cameras` |
| **Modifiers** | GP-specific modifiers (smooth, noise, tint, opacity, thickness) | Yes — `bpy.types.GpencilModifier` |
| **Geometry Nodes** | Procedural control over GP strokes (Blender 4.x+) | Yes — node trees via API |

### Animation Reuse System (Key Finding)

**Actions** are Blender's reusable animation data blocks:
- A named set of keyframes (e.g., "walk_cycle", "wave", "kiss_forehead")
- Assignable to any character with a compatible rig
- Stored as assets in the asset library (same folder system from Intent 03)
- Sequenced in the **NLA Editor** — stack, blend, and layer actions like video clips

**Pipeline:**
```
Create/download animation once
  → Save as Action (bpy.data.actions["walk_cycle"])
  → Mark as asset with tags
  → Store in shared animation library folder
  → Any compatible rig can reuse it
  → NLA Editor sequences actions per scene
```

**All scriptable via Python:**
- `bpy.data.actions` — create, access, assign actions
- `bpy.types.NlaTrack` / `NlaStrip` — sequence actions on NLA timeline
- Asset tagging via `bpy.types.AssetMetaData`

### Animation Data Management (Phase II build)

Another data management task for the custom system:
- Catalog of canned animations (walk, run, sit, talk, gesture, etc.)
- Download/import public domain animation packs
- Tag animations with semantic descriptors ("movement:walk", "emotion:happy")
- AI matches script actions to animation library ("Alice walks to Bob" → walk_cycle action)
- Same MCP-enabled pattern as storyboard system

### What Remains Manual (Phase II minimization targets)

1. **Initial character rigging** — creating the bone structure for a new character (can be templated)
2. **Custom one-off animations** — unique actions not in the library
3. **Fine-tuning** — adjusting timing, easing, overlap after automated placement
4. **Style-specific lip sync** — matching mouth shapes to audio (can be partially automated with phoneme detection)

## Outcome
- **Actual Time**: ~20 min
- **Result**: Blender's animation system is fully scriptable. Actions + NLA Editor enable animation reuse across characters and projects. Animation library management is another data task for the Phase II custom system — same MCP pattern as storyboard/asset management.
- **Follow-ups**: Phase II — build animation library manager, lip sync automation, action-to-script matching
